<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>EM算法 on 周君逸</title>
    <link>https://jzhou.org/cn/tags/em%E7%AE%97%E6%B3%95/</link>
    <description>Recent content in EM算法 on 周君逸</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 28 May 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://jzhou.org/cn/tags/em%E7%AE%97%E6%B3%95/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>阳性-无标记学习（Positive-and-Unlabeled Learning）</title>
      <link>https://jzhou.org/cn/posts/pulearning/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/cn/posts/pulearning/</guid>
      <description> 阳性-无标记学习，是指仅观察到阳性标记（记作1）而其余未标记的二元分类问题。 由于未标记的数据部分同时包含0和1，因此如果将无标记部分天真地视为0并执行传统监督学习算法将低估了正例的可能性(Ward et al. 2009; Yang et al. 2012)。但是，如果简单排除这些无标记数据，即在训练集中只有结果为1，却没有结果为0的样本，则无法直接使用已经非常成熟的监督学习方法。 为了克服这一难题，我们将在此讨论PU学习算法。
因为术语过多，请参考英文版。
参考文献 Ward, Gill, Trevor Hastie, Simon Barry, Jane Elith, and John R Leathwick. 2009. “Presence-Only Data and the Em Algorithm.” Biometrics 65 (2): 554–63.
 Yang, Peng, Xiao-Li Li, Jian-Ping Mei, Chee-Keong Kwoh, and See-Kiong Ng. 2012. “Positive-Unlabeled Learning for Disease Gene Identification.” Bioinformatics 28 (20): 2640–7.
   </description>
    </item>
    
    <item>
      <title>蒙特卡洛优化</title>
      <link>https://jzhou.org/cn/posts/mc-opt-slides/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/cn/posts/mc-opt-slides/</guid>
      <description>优化是AI / ML算法的关键组成部分。 无论提出什么目标函数，最后一步都是对其进行优化。 凸优化问题，无论是无约束的还是受约束的，都得到了很好的探讨（请参阅这里）。即使如此，优化效率或性能在很大程度上仍取决于目标函数的表面形状。 对于凸问题（convex optimization problem），解决方案可能会困在鞍点； 或对于局部凸目标函数，解决方案可能会达到局部最优而非全局最优。 当然，尝试使用不同的参数初始值是绕过此问题的一种方法。 在这里，我们介绍了另一种通过蒙特卡洛模拟找到最优值的方法，理论而言，它有机会在任何初始点达到全局最优值。
PDF 文档请点击这里.
相关的报告slides请点击这里.</description>
    </item>
    
  </channel>
</rss>
