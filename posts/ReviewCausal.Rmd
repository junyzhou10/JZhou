---
author: Junyi Zhou
date: "2022-01-01"
description: 'Review of Causal Inference: An Overview'
categories:
- Lectures/Slides
tags:
- Causal Inference
title: 'Review of Causal Inference: An Overview'

fontsize: 12pt

header-includes:
- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
- <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
- \usepackage{setspace}\doublespacing
- \usepackage{array}
- \usepackage{booktabs}
- \usepackage{multirow}
- \usepackage{threeparttable}
- \usepackage{graphicx}
- \usepackage{mathtools}
- \usepackage[most]{tcolorbox}
- \definecolor{light-yellow}{rgb}{1, 0.95, 0.7}
- \newtcolorbox{myquote}{colback=light-yellow,grow to right by=0mm,grow to left by=0mm, boxrule=0pt,boxsep=0pt,breakable}
- \newcommand{\block}[1]{\begin{myquote} \emph{#1} \end{myquote}}

indent: true

bibliography: ref.bib
biblio-style: apa
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(knitr)
library(DT)
library(xtable)
library(kableExtra)
```



![](/images/OpenJoke.png)

# Brief Introduction
The need for causal inference arises in many different fields. Economists want to quantify the causal effects of interest rate cut on the economy. Trade policy makers want to know whether increased tariff causes changes in trade deficit. Healthcare providers want to assess whether a therapy causes changes in specific patient outcomes. Conceptually, causal inference is all about a question of 'what if'. People can ask themself *what if* I took chemotherapy rather than radiation to treat my cancer, would I get better now? Or *what if* I went to graduate school rather entering the job market after graduation, would I have a better career prospects? The economists can be curious about *what if* the Federal Reserve raised the interest rate by 50bp rather than 25bp, would the inflation rate be better controlled? The most straightforward way to answer those questions is, *if* we know what will happen for any of these actions, we of course can find the one benefit our goal the most. This leads to the **Potential Outcome Framework** in the next section.


# Potential Outcome Framework 

Potential outcome framework, or Rubin-Neyman potential outcome framework [@rubin1974estimating; @neyman1923application; @imbens2015causal], is the most canonical framework for causal inference studies. Suppose the treatment has binary levels, denoted as $T \in \{1,-1\}$, and the corresponding *potential outcomes* are $Y^{(1)}$ and $Y^{(-1)}$, respectively. But people can only observe one realized outcome, i.e., the observed outcome is **factual** and unobserved outcome is **counterfactual**. In other words, the outcome can only be either $Y^{(1)}$ or $Y^{(-1)}$, which is represented by
$$
Y = 1[T = 1]Y^{(1)} + 1[T = -1]Y^{(-1)}.
$$
The individual causal effect, or individual treatment effect (ITE), is thus defined by
\begin{equation}
\tau_i = Y_i^{(1)} - Y_i^{(-1)} \label{eq:ITE}
\end{equation}
for subject $i$, but it is unobservable in real world.

To make the estimation of causal effect feasible, we have to make several assumptions:

**Assumption 1. Stable Unit Treatment Value Assumption (SUTVA)** [@rubin1980randomization] \
*1) Treatment applied to one unit does not affect the outcome for other units* **(No Interference)** \
*2) There is no hidden variation, for example, different forms or versions of each treatment level, for a single treatment* **(Consistency)**\

**Assumption 2. Unconfoundedness/Ignorability/Strongly Ignorable Treatment Assignment (SITA)**
\begin{equation}
(Y^{(1)}, Y^{(-1)}) \perp T \mid \mathbf X, \label{assump2}
\end{equation}<!-- \tag{Assumption 2} --> 

**Assumption 3. Common Support/Positivity**
\begin{equation}
0 < \Pr(T = 1 | \mathbf X = \mathbf x) =\pi(\mathbf x) < 1 \label{assump3}
\end{equation}<!-- \tag{Assumption 3} --> 
where $\pi(\mathbf x)$ is called **Propensity Score (PS)**.

\block{REMARK: There is always a trade-off between Unconfoundedness and Positivity (think of why).}

In the following sections, we will discuss how to estimate of the individual or sample average treatment effects under this potential outcome framework with these assumptions.



# A Big Picture of Causal Inference
![](/images/CausalFrame.png)

In general, most research on causal inference either studies the population-wise treatment effect, i.e., the average treatment effect (ATE), or the subject-level/individual treatment effect (ITE), i.e., the conditional treatment effect (CATE), which is exchangeable to heterogenuous treatment effect (HTE). 

Basically, the observations can be either from randomized experiments, like randomized controlled trial (RCT), or observational studies, so-called real world data (RWD). With different observations, the estimation methods can be very different. 

Briefly speaking, the ATE can be easily obtained via well-designed RCTs or A/B tests (we will see why in section [Average Treatment Effect](#ATE)). That's how pharmaceutical/biotech companies evaluate their drug effectiveness. In recent years, a big trend in drug development which is [supported by FDA](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/considerations-use-real-world-data-and-real-world-evidence-support-regulatory-decision-making-drug) is to use RWD and [real world evidence (RWE)](https://www.fda.gov/science-research/science-and-research-special-topics/real-world-evidence) to support as well as speed-up the regulatory approval. Since RWD is obervational rather than randomized controlled, a sizable methods are proposed including **Matching** [@rosenbaum1983central; @austin2014comparison], **Subclassification** [@rosenbaum1984reducing], **Weighting** [@hirano2003efficient; @austin2011introduction; @austin2015moving], **Regression** [@rubin1979using; @rubin:1985; @hahn1998role; @gutman2013robust], or a mixture of them [@bang2005doubly; @van2006targeted].

Another heat topic in causal inference is about the estimation of CATE/HTE/ITE from observational studies. This is usually tough because given the noise of observational studies and subject-level estimation, the estimates can have large variance. Various of methods have been proposed including but not limited to causal boosting [@powers2018some], causal forests [@athey2016recursive], individual-treatment-rule-based methods that mostly involves outcome weighted learning (OWL) [@qian2011performance; @zhao2012estimating; @zhou2017residual; @qi2020multi], and several meta-learners, such as X-learner [@kunzel2019metalearners] and R-learner [@NieQuasi2020]. These methods can be easily degenerate to study the CATE/HTE/ITE in randomized trials. 



## Causal Estimands
Different scientific questions leads to the estimation of different causal estimands. As aforementioned, for instance, ATE is basically for assessing the population/sub-population treatment effect which is the key focus for the drug companies. On the other hand, CATE which is more about individual-level treatment effect, so it can be used to do precision medicine or personalized recommendations. Here is a list of some commonly used acronym causal estimands:

- ITE (individual treatment effect)
- ATE (average treatment effect)
  - PATE (population average treatment effect)
  - SATE (sample average treatment effect)
- ATT (average treatment effect for the treated)
- ATU/ATC (average treatment effect for the untreated/control)
- CATE (conditional treatment effect), HTE (heterogeneous treatment effect), Local ATE
- CATT (conditional treatment effect for the treated)
- CATU/CATC (conditional average treatment effect for the untreated/control)

# Average Treatment Effect (ATE) {#ATE} 
## Randomized Controlled Experiments
ATE can be observed from well-designed randomized controlled trial (RCT) directly, which reveals the population-wise treatment effect.
$$
\begin{aligned}
\tau & = E[Y^{(1)} - Y^{(-1)}]\\
& = E[Y^{(1)}] - E[Y^{(-1)}] \quad\text{ (causation)}\\
& = E[Y^{(1)} \mid T = 1] - E[Y^{(-1)} \mid T = -1] \\
& = E[Y \mid T = 1] - E[Y \mid T = -1] \quad\text{ (association)}
\end{aligned}
$$

\block{1. Think about why and when the equation holds; \newline 2. What if trial has Response-adaptive Randomization (RAR)}

The essence here is the randomization eliminates all the confounding effects, or in other words, the **Unconfoundedness** and **Positivity** assumptions holds automatically under randomized controlled experiments. 

Another commonly used causal estimand is average treatment effect for the treated (ATT), which focus on the population receive the treatment 
$$
\tau^{\text{ATT}} = E[Y^{(1)} - Y^{(-1)}\mid T = 1].
$$

***

## Observational Studies
Propensity score (PS) [@rosenbaum1983central] is defined as the conditional probability of receiving a treatment given pre-treatment covariates $\mathbf X$. PS is the very core part in doing causal analysis, especially in the observational studies (but even in randomized trials, PS can be used to do some covariates adjustment for better results). 

Though **Matching** is also a very popular approach in ATE estimation, here we discuss **Weighting** in details.

### Matching(subclassification/coarsened)
The very beginning idea of estimating ATE (under the Assumption 1, 2 and 3) by first finding pairs of observations with covariates *close enough* to each other in two arms, and then average them out:
$$
\tau = E_{\mathcal X}\{E[Y|\mathbf X \in \mathcal X_j, T = 1]-E[Y|\mathbf X\in \mathcal X_j, T = -1]\}
$$
where $\mathcal X_j$ is a subspace such that $\bigcup_{j=1}^J \mathcal X_j = \mathcal X$ and $\mathcal X_i \bigcap \mathcal X_j = \phi$ for any $i\neq j$. One-to-one paired matching is hard to find, especially when $\mathcal X$ is of high dimension. So people propose to use subclassification or coarsened, which also provide more robust estimations.

### Matching by PS
The matching idea is finally ruled by the milestone work from @rosenbaum1983central who demonstrated that only matching by propensity score is good enough, because
$$
(Y^{(1)}, Y^{(-1)}) \perp T \mid \mathbf X \Rightarrow (Y^{(1)}, Y^{(-1)}) \perp T \mid \pi(\mathbf X ).
$$
The whole matching task is transformed from a potential high diemensional covariate space $\mathcal X$ to a linear scalar. 
$$
\begin{aligned}
& E_{\pi(\mathbf X)}\{E[Y \mid T = 1, \pi(\mathbf X)] - E[Y \mid T = -1, \pi(\mathbf X)] \} \\
=& E_{\pi(\mathbf X)}\{E[Y^{(1)} \mid \pi(\mathbf X)] - E[Y^{(-1)} \mid \pi(\mathbf X)]\} \\
=& E[Y^{(1)} - Y^{(0)}] = \tau
\end{aligned}
$$
But how to achieve a good estimator of propensity score $\pi(\cdot)$ turns out to be the key problem.  \newline
\block{Though the CATE quantity occurs during the estimation process of ATE, why it is not suitable for CATE estimation?}


### Inverse Probability Weighting (IPW)
It can be shown that (Proof in Appendix \ref{app1})
$$
E[Y^{(t)}] = E\left[\frac{1[T = t]Y}{Pr(T=t \mid \mathbf X)}\right]
$$
which leads to
\begin{equation}
\tau = E[Y^{(1)} - Y^{(-1)}] = E\left[\frac{1[T = 1]Y}{\pi(\mathbf X)}\right] - E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right]. \label{eq:IPW}
\end{equation}
Thus, the estimator of ATE based on observed dataset is
$$
\hat\tau = \frac{1}{n}\sum_{i:t_i =1} \frac{y_i}{\hat \pi(\mathbf x_i)} - \frac{1}{n}\sum_{i:t_i =-1} \frac{y_i}{1-\hat \pi(\mathbf x_i)}.
$$
As this approach can be traced back to @horvitz1952generalization, sometimes it is refered to as Horvitz–Thompson (HT) estimator  [@imai2014covariate]. Since directly weighting on the inverse of propensity score may cause large variability when estimated $\pi(\mathbf x_i)$ is either close to 1 or 0, @hirano2003efficient propose to standardize the weight by
$$
\hat\tau = \sum_{i:t_i =1} \frac{y_i}{\hat \pi(\mathbf x_i)}\Big/\sum_{i:t_i =1}\frac{1}{\hat \pi(\mathbf x_i)} - \sum_{i:t_i =-1} \frac{y_i}{1-\hat \pi(\mathbf x_i)}\Big/\sum_{i:t_i =-1}\frac{1}{1-\hat \pi(\mathbf x_i)}.
$$
In our simulation studies, to distinguish from HT, IPW is refered to this standardized weighting method.
\newline
Similarly, for ATT, it can be estimated by
$$
\begin{aligned}
\tau^{\text{ATT}}
=& E[Y^{(1)} - Y^{(-1)}\mid T = 1] = E[Y^{(1)}\mid T=1] - E[Y^{(-1)}\mid T=1]\\
=& \frac{1}{\pi}\left(E[1[T=1]Y^{(1)}]+E[1[T=-1]Y^{(-1)}] - E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right]\right) \\
=& \frac{1}{\pi}E\left[\left(1-\frac{1[T = -1]}{1-\pi(\mathbf X)}\right)Y\right] 
= \frac{1}{\pi}E\left[\left(\frac{1[T = 1]-\pi(\mathbf X)}{1-\pi(\mathbf X)}\right)Y\right] \\
=& \frac{1}{n_1}\left(\sum_{i:t_i=1}y_i - \sum_{i:t_i=-1}\frac{\pi(\mathbf x_i)}{1-\pi(\mathbf x_i)}y_i\right) \quad \text{for finite sample estimation}
\end{aligned} 
$$
where the second equation comes from:
$$
\begin{aligned}
& E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right] = E[Y^{(-1)}] = E[Y^{(-1)}\mid T=1]\Pr(T=1) + E[Y^{(-1)}\mid T=-1]\Pr(T=-1)\ \\
\Rightarrow & E[Y^{(-1)}\mid T=1] = \frac{1}{\pi}\left(E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right] - E[1[T=-1]Y^{(-1)}]\right)
\end{aligned}
$$


### Augmented Inverse Probability Weighting (AIPW) \label{sec:AIPW} 
@robins1995analysis augmented the IPW by a weighted average of the outcome model
\begin{align}
\tau^{\text{AIPW}} =& \underbrace{E\left[\frac{1[T = 1]Y}{\pi(\mathbf X)}\right] - E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right]}_\text{IPW} \nonumber \\
&- \underbrace{E\left[\frac{1[T=1] -\pi(\mathbf X)}{\pi(\mathbf X)}\mu_1(\mathbf X) + \frac{1[T=1] -\pi(\mathbf X)}{1-\pi(\mathbf X)} \mu_{-1}(\mathbf X)\right]}_\text{Augmentation} \label{eq:AIPW}
\end{align}
This AIPW is called “doubly robust” because it is consistent as long as either the treatment assignment mechanism or the outcome model is correctly specified [@kurz2021augmented]. If the propensity score is correctly specified, then $E(1[T=t] - \Pr(T=t\mid \mathbf X)) = E\left[E(1[T=t] - \Pr(T=t\mid \mathbf X)\mid \mathbf X)\right] = 0$ which simplifies AIPW to IPW no matter whether response surface functions $\mu_T()$ are correct. On the other hand, if response model $\mu_T()$ are correctly specified but propensity score is not correctly estimated, AIPW reduces to S- or T-learner (definitions are in the next section. Proof in Appendix \ref{app2})









# Appendix
## Proof of IPW \label{app1}
$$
\begin{aligned}
& E\left[\frac{1[T = t]Y}{\Pr(T=t \mid \mathbf X)}\right] \\
=& E\left[ E\left(\frac{1[T = t]Y}{\Pr(T=t \mid \mathbf X)} \mid \mathbf X\right)\right] \\
=& E\left[ \frac{E\left(1[T = t]\sum_c1[T=c]Y^{(c)} \mid \mathbf X \right)}{\Pr(T=t \mid \mathbf X)} \right] \\
=& E\left[ \frac{\sum_a E\left(1[T = t]\sum_c1[T=c]Y^{(c)} \mid T = a, \mathbf X \right)\Pr(T = a\mid \mathbf X)}{\Pr(T=t \mid \mathbf X)} \right] \\
=& E\left[ \frac{E\left(Y^{(t)} \mid T = t, \mathbf X \right)\Pr(T = t\mid \mathbf X)}{\Pr(T=t \mid \mathbf X)} \right] \\
=& E\left[ E\left(Y^{(t)} \mid T = t, \mathbf X \right) \right] \\
=& E\left[ E\left(Y^{(t)} \mid \mathbf X \right) \right] \text{ (unconfoundedness)} \\
=& E\left[ Y^{(t)} \right].
\end{aligned}
$$

## Proof of AIPW \label{app2}
To demonstrate that AIPW can be consistent even when propensity score is not correctly specified, we only need to focus on the general element $E\left[\frac{1[T = t]Y}{\pi^{(t)}(\mathbf X)} - \frac{1[T=t] -\pi^{(t)}(\mathbf X)}{\pi^{(t)}(\mathbf X)} \mu_t(\mathbf X)\right]$.

When propensity score is wrong, i.e., $\pi^{(t)}(\mathbf X)\neq \Pr(T = t\mid \mathbf X)$ but response surface estimation is correct, i.e., $\mu_t(\mathbf X) = E(Y^{(t)} \mid \mathbf X)$,
$$
\begin{aligned}
& E\left[\frac{1[T = t]Y}{\pi^{(t)}(\mathbf X)} - \frac{1[T=t]}{\pi^{(t)}(\mathbf X)} \mu_t(\mathbf X) + \mu_t(\mathbf X)\right] \\
=& E\left[E\left(\frac{1[T = t]Y}{\pi^{(t)}(\mathbf X)} - \frac{1[T=t]}{\pi^{(t)}(\mathbf X)} \mu_t(\mathbf X) + \mu_t(\mathbf X) \mid \mathbf X \right)\right] \\
=& E\left[ \frac{E\left(1[T = t](Y - \mu_t(\mathbf X)) \mid \mathbf X\right)}{\pi^{(t)}(\mathbf X)}\right] + E[E(Y^{(t)} \mid \mathbf X)] \\
=& E\left[ \frac{\sum_a E\left(1[T = t](Y - \mu_t(\mathbf X)) \mid T = a,\mathbf X\right)\Pr(T = a \mid \mathbf X)}{\pi^{(t)}(\mathbf X)}\right] + E\left[Y^{(t)}\right]  \\
=& E\left[ \frac{E\left(Y - \mu_t(\mathbf X) \mid T = t,\mathbf X\right)\Pr(T = t \mid \mathbf X)}{\pi^{(t)}(\mathbf X)}\right] + E\left[Y^{(t)}\right]  \\
=& E\left[ \frac{\Pr(T = t \mid \mathbf X)}{\pi^{(t)}(\mathbf X)}\underbrace{\left(E\left(Y^{(t)} \mid \mathbf X\right) - \mu_t(\mathbf X) \right)}_{=0} \right] + E\left[Y^{(t)}\right]  \text{ (unconfoundedness)} \\
=& E\left[Y^{(t)}\right] .
\end{aligned}
$$
Therefore, under this situation, AIPW in equation (\ref{eq:AIPW}) becomes $\tau^{AIPW}=E\left[Y^{(1)}\right] - E\left[Y^{(-1)}\right] = \tau$.



# References
\small
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}
\noindent


