<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Junyi Zhou</title>
    <link>https://jzhou.org/posts/</link>
    <description>Recent content in Posts on Junyi Zhou</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 21 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://jzhou.org/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Canonical Correlation Analysis (CCA)</title>
      <link>https://jzhou.org/posts/cca/</link>
      <pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/posts/cca/</guid>
      <description>.watch-out { background-color: AliceBlue; border: 3px solid AliceBlue; font-weight: bold; }  Motivation of canonical correlation analysis (CCA) The main motivation of CCA is to provide multi-view of the data, which usually consists of two sets of variables. Canonical correlation analysis determines a set of canonical variates, i.e., orthogonal linear combinations of the variables within each set that best explain the variability both within and between sets. So there could be multiple dimensions (canonical variables) that represent the association between two sets of variables.</description>
    </item>
    
    <item>
      <title>Clustering Longitudinal Data (Online App)</title>
      <link>https://jzhou.org/posts/clusterlongapp/</link>
      <pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/posts/clusterlongapp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ClusterLong: An R-package to Cluster Longitudinal Data</title>
      <link>https://jzhou.org/posts/clusterlongpackage/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/posts/clusterlongpackage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Positive-and-Unlabeled Learning</title>
      <link>https://jzhou.org/posts/pulearning/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/posts/pulearning/</guid>
      <description>Introduction Positive and unlabeled learning, or positive-unlabeled (PU) learning, refers to the binary classification problem where only positive labels are observed and the rest are unlabeled. Since unlabeled part of data consists of both positive and negative instances, naively treating them as negative and performing a standard classification learning algorithm will underestimate the probability of being positive (Ward et al. 2009; Yang et al. 2012). Without providing negative instances in the training set, however, will prevent the direct use of well-developed supervised classification methods.</description>
    </item>
    
    <item>
      <title>Connection between the Idea behind OptTrialDesign &amp; CAPM in Finance</title>
      <link>https://jzhou.org/posts/odt-capm/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/posts/odt-capm/</guid>
      <description>Idea behind OptTrialDesign In Optimal Design Strategy, we propose a novel way to find the optimal trial design, in terms of maximizing the financial outcomes, i.e., Expected Net Revenue (ENR), but also under the regulatory requirements. We have to trade-off because in pharmaceutical companiesâ€™ position, a shorter study/trial duration leads to earlier market access which helps to maximize the profits before LOE (loss of exclusivity). On the other hand, regulatory agencies and health technology assessment (HTA) bodies wish to see more guaranteed clinical results, which usually require longer study duration.</description>
    </item>
    
    <item>
      <title>Review of Causal Inference</title>
      <link>https://jzhou.org/posts/proposal/</link>
      <pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/posts/proposal/</guid>
      <description>Notice: the causal inference described here follows the idea of Potential Outcome Framework (Neyman 1923; Rubin 1974). For another big camp of causal inference studies, i.e., Structural Causal Model (SCM) (Pearl 2009), please refer to this well-written paper here.
Heterogeneous Treatment Effect Estimation 1. Introduction The need for causal inference arises in many different fields. Economists want to quantify the causal effects of interest rate cut on the economy.</description>
    </item>
    
    <item>
      <title>Optimal Trial Design (Online App)</title>
      <link>https://jzhou.org/posts/opttrialdesignapp/</link>
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/posts/opttrialdesignapp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>OptTrialDesign: An R-package for Optimal Trial Design--Balancing between Financial Outcomes and Regulatory Concerns</title>
      <link>https://jzhou.org/posts/opttrialdesignpackage/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/posts/opttrialdesignpackage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Monte Carlo Optimization</title>
      <link>https://jzhou.org/posts/mc-opt-slides/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jzhou.org/posts/mc-opt-slides/</guid>
      <description>Optimization is the key component of the AI/ML algorithm. Whatever objective function is proposed, the final step is to optimize it. Convex optimization problems, either unconstrained or constrained, are well explored (see here). Even though, the efficiency or performance still largely depends on the shape of the surface of the objective function. For convex problems, the solution could be trapped at saddle points; or for local convex objective functions, the solution might reach to the local optima instead of global optima.</description>
    </item>
    
  </channel>
</rss>
