<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">
    <meta name="color-scheme" content="light dark">

    

    <meta name="author" content="Junyi Zhou">
    <meta name="description" content="Review of Causal Inference: An Overview">
    <meta name="keywords" content="personal, projects, apps">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Review of Causal Inference: An Overview"/>
<meta name="twitter:description" content="Review of Causal Inference: An Overview"/>

    <meta property="og:title" content="Review of Causal Inference: An Overview" />
<meta property="og:description" content="Review of Causal Inference: An Overview" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jzhou.org/posts/reviewcausal/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-01-01T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-01-01T00:00:00&#43;00:00" />



    <title>
  Review of Causal Inference: An Overview · Junyi Zhou
</title>

    
      <link rel="canonical" href="https://jzhou.org/posts/reviewcausal/">
    

    <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.1.7" as="font" type="font/woff2" crossorigin>

    
      
      
      <link rel="stylesheet" href="/css/coder.min.406d0bb9b7e93dd1c4497ee4abb177af6bea8f6c16aea89ae05f2aef56ef44e5.css" integrity="sha256-QG0LubfpPdHESX7kq7F3r2vqj2wWrqia4F8q71bvROU=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="/css/coder-dark.min.dde8a61eb31a32353b4baf3d9113f03c4ea2a8ca9bb736f59ca2d2b2cb664f0b.css" integrity="sha256-3eimHrMaMjU7S689kRPwPE6iqMqbtzb1nKLSsstmTws=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    

    <meta name="generator" content="Hugo 0.83.0" />
  </head>

  
  
    
  
  <body class="preload-transitions colorscheme-auto"
        onload=""
  >
    
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Junyi Zhou
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/publications/">Publications</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
          
          
          
            
          
            
              
                <li class="navigation-item menu-separator">
                  <span>|</span>
                </li>
                
              
              <li class="navigation-item">
                <a href="https://jzhou.org/cn/">中文</a>
              </li>
            
          
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://jzhou.org/posts/reviewcausal/">
              Review of Causal Inference: An Overview
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2022-01-01T00:00:00Z'>
                January 1, 2022
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              13-minute read
            </span>
          </div>
          
          <div class="categories">
  <i class="fa fa-folder" aria-hidden="true"></i>
    <a href="/categories/lectures/slides/">Lectures/Slides</a></div>

          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <a href="/tags/causal-inference/">Causal Inference</a></div>

        </div>
      </header>

      <div>
        
        
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>













<p><img src="/images/OpenJoke.png" /></p>
<div id="brief-introduction" class="section level1">
<h1>Brief Introduction</h1>
<p>The need for causal inference arises in many different fields. Economists want to quantify the causal effects of interest rate cut on the economy. Trade policy makers want to know whether increased tariff causes changes in trade deficit. Healthcare providers want to assess whether a therapy causes changes in specific patient outcomes. Conceptually, causal inference is all about a question of ‘what if’. People can ask themself <em>what if</em> I took chemotherapy rather than radiation to treat my cancer, would I get better now? Or <em>what if</em> I went to graduate school rather entering the job market after graduation, would I have a better career prospects? The economists can be curious about <em>what if</em> the Federal Reserve raised the interest rate by 50bp rather than 25bp, would the inflation rate be better controlled? The most straightforward way to answer those questions is, <em>if</em> we know what will happen for any of these actions, we of course can find the one benefit our goal the most. This leads to the <strong>Potential Outcome Framework</strong> in the next section.</p>
</div>
<div id="potential-outcome-framework" class="section level1">
<h1>Potential Outcome Framework</h1>
<p>Potential outcome framework, or Rubin-Neyman potential outcome framework <span class="citation">(Rubin <a href="#ref-rubin1974estimating" role="doc-biblioref">1974</a>; Neyman <a href="#ref-neyman1923application" role="doc-biblioref">1923</a>; Imbens and Rubin <a href="#ref-imbens2015causal" role="doc-biblioref">2015</a>)</span>, is the most canonical framework for causal inference studies. Suppose the treatment has binary levels, denoted as <span class="math inline">\(T \in \{1,-1\}\)</span>, and the corresponding <em>potential outcomes</em> are <span class="math inline">\(Y^{(1)}\)</span> and <span class="math inline">\(Y^{(-1)}\)</span>, respectively. But people can only observe one realized outcome, i.e., the observed outcome is <strong>factual</strong> and unobserved outcome is <strong>counterfactual</strong>. In other words, the outcome can only be either <span class="math inline">\(Y^{(1)}\)</span> or <span class="math inline">\(Y^{(-1)}\)</span>, which is represented by
<span class="math display">\[
Y = 1[T = 1]Y^{(1)} + 1[T = -1]Y^{(-1)}.
\]</span>
The individual causal effect, or individual treatment effect (ITE), is thus defined by
<span class="math display">\[\begin{equation}
\tau_i = Y_i^{(1)} - Y_i^{(-1)} \label{eq:ITE}
\end{equation}\]</span>
for subject <span class="math inline">\(i\)</span>, but it is unobservable in real world.</p>
<p>To make the estimation of causal effect feasible, we have to make several assumptions:</p>
<p><strong>Assumption 1. Stable Unit Treatment Value Assumption (SUTVA)</strong> <span class="citation">(Rubin <a href="#ref-rubin1980randomization" role="doc-biblioref">1980</a>)</span><br />
<em>1) Treatment applied to one unit does not affect the outcome for other units</em> <strong>(No Interference)</strong><br />
<em>2) There is no hidden variation, for example, different forms or versions of each treatment level, for a single treatment</em> <strong>(Consistency)</strong><br />
</p>
<p><strong>Assumption 2. Unconfoundedness/Ignorability/Strongly Ignorable Treatment Assignment (SITA)</strong>
<span class="math display">\[\begin{equation}
(Y^{(1)}, Y^{(-1)}) \perp T \mid \mathbf X, \label{assump2}
\end{equation}\]</span><!-- \tag{Assumption 2} --></p>
<p><strong>Assumption 3. Common Support/Positivity</strong>
<span class="math display">\[\begin{equation}
0 &lt; \Pr(T = 1 | \mathbf X = \mathbf x) =\pi(\mathbf x) &lt; 1 \label{assump3}
\end{equation}\]</span><!-- \tag{Assumption 3} -->
where <span class="math inline">\(\pi(\mathbf x)\)</span> is called <strong>Propensity Score (PS)</strong>.</p>

<p>In the following sections, we will discuss how to estimate of the individual or sample average treatment effects under this potential outcome framework with these assumptions.</p>
</div>
<div id="a-big-picture-of-causal-inference" class="section level1">
<h1>A Big Picture of Causal Inference</h1>
<p><img src="/images/CausalFrame.png" /></p>
<p>In general, most research on causal inference either studies the population-wise treatment effect, i.e., the average treatment effect (ATE), or the subject-level/individual treatment effect (ITE), i.e., the conditional treatment effect (CATE), which is exchangeable to heterogenuous treatment effect (HTE).</p>
<p>Basically, the observations can be either from randomized experiments, like randomized controlled trial (RCT), or observational studies, so-called real world data (RWD). With different observations, the estimation methods can be very different.</p>
<p>Briefly speaking, the ATE can be easily obtained via well-designed RCTs or A/B tests (we will see why in section <a href="#ATE">Average Treatment Effect</a>). That’s how pharmaceutical/biotech companies evaluate their drug effectiveness. In recent years, a big trend in drug development which is <a href="https://www.fda.gov/regulatory-information/search-fda-guidance-documents/considerations-use-real-world-data-and-real-world-evidence-support-regulatory-decision-making-drug">supported by FDA</a> is to use RWD and <a href="https://www.fda.gov/science-research/science-and-research-special-topics/real-world-evidence">real world evidence (RWE)</a> to support as well as speed-up the regulatory approval. Since RWD is obervational rather than randomized controlled, a sizable methods are proposed including <strong>Matching</strong> <span class="citation">(Rosenbaum and Rubin <a href="#ref-rosenbaum1983central" role="doc-biblioref">1983</a>; Austin <a href="#ref-austin2014comparison" role="doc-biblioref">2014</a>)</span>, <strong>Subclassification</strong> <span class="citation">(Rosenbaum and Rubin <a href="#ref-rosenbaum1984reducing" role="doc-biblioref">1984</a>)</span>, <strong>Weighting</strong> <span class="citation">(Hirano, Imbens, and Ridder <a href="#ref-hirano2003efficient" role="doc-biblioref">2003</a>; Austin <a href="#ref-austin2011introduction" role="doc-biblioref">2011</a>; Austin and Stuart <a href="#ref-austin2015moving" role="doc-biblioref">2015</a>)</span>, <strong>Regression</strong> <span class="citation">(Rubin <a href="#ref-rubin1979using" role="doc-biblioref">1979</a>, <a href="#ref-rubin:1985" role="doc-biblioref">1985</a>; Hahn <a href="#ref-hahn1998role" role="doc-biblioref">1998</a>; Gutman and Rubin <a href="#ref-gutman2013robust" role="doc-biblioref">2013</a>)</span>, or a mixture of them <span class="citation">(Bang and Robins <a href="#ref-bang2005doubly" role="doc-biblioref">2005</a>; Van Der Laan and Rubin <a href="#ref-van2006targeted" role="doc-biblioref">2006</a>)</span>.</p>
<p>Another heat topic in causal inference is about the estimation of CATE/HTE/ITE from observational studies. This is usually tough because given the noise of observational studies and subject-level estimation, the estimates can have large variance. Various of methods have been proposed including but not limited to causal boosting <span class="citation">(Powers et al. <a href="#ref-powers2018some" role="doc-biblioref">2018</a>)</span>, causal forests <span class="citation">(Athey and Imbens <a href="#ref-athey2016recursive" role="doc-biblioref">2016</a>)</span>, individual-treatment-rule-based methods that mostly involves outcome weighted learning (OWL) <span class="citation">(Qian and Murphy <a href="#ref-qian2011performance" role="doc-biblioref">2011</a>; Zhao et al. <a href="#ref-zhao2012estimating" role="doc-biblioref">2012</a>; Zhou et al. <a href="#ref-zhou2017residual" role="doc-biblioref">2017</a>; Qi et al. <a href="#ref-qi2020multi" role="doc-biblioref">2020</a>)</span>, and several meta-learners, such as X-learner <span class="citation">(Künzel et al. <a href="#ref-kunzel2019metalearners" role="doc-biblioref">2019</a>)</span> and R-learner <span class="citation">(Nie and Wager <a href="#ref-NieQuasi2020" role="doc-biblioref">2020</a>)</span>. These methods can be easily degenerate to study the CATE/HTE/ITE in randomized trials.</p>
<div id="causal-estimands" class="section level2">
<h2>Causal Estimands</h2>
<p>Different scientific questions leads to the estimation of different causal estimands. As aforementioned, for instance, ATE is basically for assessing the population/sub-population treatment effect which is the key focus for the drug companies. On the other hand, CATE which is more about individual-level treatment effect, so it can be used to do precision medicine or personalized recommendations. Here is a list of some commonly used acronym causal estimands:</p>
<ul>
<li>ITE (individual treatment effect)</li>
<li>ATE (average treatment effect)
<ul>
<li>PATE (population average treatment effect)</li>
<li>SATE (sample average treatment effect)</li>
</ul></li>
<li>ATT (average treatment effect for the treated)</li>
<li>ATU/ATC (average treatment effect for the untreated/control)</li>
<li>CATE (conditional treatment effect), HTE (heterogeneous treatment effect), Local ATE</li>
<li>CATT (conditional treatment effect for the treated)</li>
<li>CATU/CATC (conditional average treatment effect for the untreated/control)</li>
</ul>
</div>
</div>
<div id="ATE" class="section level1">
<h1>Average Treatment Effect (ATE)</h1>
<div id="randomized-controlled-experiments" class="section level2">
<h2>Randomized Controlled Experiments</h2>
<p>ATE can be observed from well-designed randomized controlled trial (RCT) directly, which reveals the population-wise treatment effect.
<span class="math display">\[
\begin{aligned}
\tau &amp; = E[Y^{(1)} - Y^{(-1)}]\\
&amp; = E[Y^{(1)}] - E[Y^{(-1)}] \quad\text{ (causation)}\\
&amp; = E[Y^{(1)} \mid T = 1] - E[Y^{(-1)} \mid T = -1] \\
&amp; = E[Y \mid T = 1] - E[Y \mid T = -1] \quad\text{ (association)}
\end{aligned}
\]</span></p>

<p>The essence here is the randomization eliminates all the confounding effects, or in other words, the <strong>Unconfoundedness</strong> and <strong>Positivity</strong> assumptions holds automatically under randomized controlled experiments.</p>
<p>Another commonly used causal estimand is average treatment effect for the treated (ATT), which focus on the population receive the treatment
<span class="math display">\[
\tau^{\text{ATT}} = E[Y^{(1)} - Y^{(-1)}\mid T = 1].
\]</span></p>
<hr />
</div>
<div id="observational-studies" class="section level2">
<h2>Observational Studies</h2>
<p>Propensity score (PS) <span class="citation">(Rosenbaum and Rubin <a href="#ref-rosenbaum1983central" role="doc-biblioref">1983</a>)</span> is defined as the conditional probability of receiving a treatment given pre-treatment covariates <span class="math inline">\(\mathbf X\)</span>. PS is the very core part in doing causal analysis, especially in the observational studies (but even in randomized trials, PS can be used to do some covariates adjustment for better results).</p>
<p>Though <strong>Matching</strong> is also a very popular approach in ATE estimation, here we discuss <strong>Weighting</strong> in details.</p>
<div id="matchingsubclassificationcoarsened" class="section level3">
<h3>Matching(subclassification/coarsened)</h3>
<p>The very beginning idea of estimating ATE (under the Assumption 1, 2 and 3) by first finding pairs of observations with covariates <em>close enough</em> to each other in two arms, and then average them out:
<span class="math display">\[
\tau = E_{\mathcal X}\{E[Y|\mathbf X \in \mathcal X_j, T = 1]-E[Y|\mathbf X\in \mathcal X_j, T = -1]\}
\]</span>
where <span class="math inline">\(\mathcal X_j\)</span> is a subspace such that <span class="math inline">\(\bigcup_{j=1}^J \mathcal X_j = \mathcal X\)</span> and <span class="math inline">\(\mathcal X_i \bigcap \mathcal X_j = \phi\)</span> for any <span class="math inline">\(i\neq j\)</span>. One-to-one paired matching is hard to find, especially when <span class="math inline">\(\mathcal X\)</span> is of high dimension. So people propose to use subclassification or coarsened, which also provide more robust estimations.</p>
</div>
<div id="matching-by-ps" class="section level3">
<h3>Matching by PS</h3>
<p>The matching idea is finally ruled by the milestone work from <span class="citation">Rosenbaum and Rubin (<a href="#ref-rosenbaum1983central" role="doc-biblioref">1983</a>)</span> who demonstrated that only matching by propensity score is good enough, because
<span class="math display">\[
(Y^{(1)}, Y^{(-1)}) \perp T \mid \mathbf X \Rightarrow (Y^{(1)}, Y^{(-1)}) \perp T \mid \pi(\mathbf X ).
\]</span>
The whole matching task is transformed from a potential high diemensional covariate space <span class="math inline">\(\mathcal X\)</span> to a linear scalar.
<span class="math display">\[
\begin{aligned}
&amp; E_{\pi(\mathbf X)}\{E[Y \mid T = 1, \pi(\mathbf X)] - E[Y \mid T = -1, \pi(\mathbf X)] \} \\
=&amp; E_{\pi(\mathbf X)}\{E[Y^{(1)} \mid \pi(\mathbf X)] - E[Y^{(-1)} \mid \pi(\mathbf X)]\} \\
=&amp; E[Y^{(1)} - Y^{(0)}] = \tau
\end{aligned}
\]</span>
But how to achieve a good estimator of propensity score <span class="math inline">\(\pi(\cdot)\)</span> turns out to be the key problem. 
</p>
</div>
<div id="inverse-probability-weighting-ipw" class="section level3">
<h3>Inverse Probability Weighting (IPW)</h3>
<p>It can be shown that (Proof in Appendix )
<span class="math display">\[
E[Y^{(t)}] = E\left[\frac{1[T = t]Y}{Pr(T=t \mid \mathbf X)}\right]
\]</span>
which leads to
<span class="math display">\[\begin{equation}
\tau = E[Y^{(1)} - Y^{(-1)}] = E\left[\frac{1[T = 1]Y}{\pi(\mathbf X)}\right] - E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right]. \label{eq:IPW}
\end{equation}\]</span>
Thus, the estimator of ATE based on observed dataset is
<span class="math display">\[
\hat\tau = \frac{1}{n}\sum_{i:t_i =1} \frac{y_i}{\hat \pi(\mathbf x_i)} - \frac{1}{n}\sum_{i:t_i =-1} \frac{y_i}{1-\hat \pi(\mathbf x_i)}.
\]</span>
As this approach can be traced back to <span class="citation">Horvitz and Thompson (<a href="#ref-horvitz1952generalization" role="doc-biblioref">1952</a>)</span>, sometimes it is refered to as Horvitz–Thompson (HT) estimator <span class="citation">(Imai and Ratkovic <a href="#ref-imai2014covariate" role="doc-biblioref">2014</a>)</span>. Since directly weighting on the inverse of propensity score may cause large variability when estimated <span class="math inline">\(\pi(\mathbf x_i)\)</span> is either close to 1 or 0, <span class="citation">Hirano, Imbens, and Ridder (<a href="#ref-hirano2003efficient" role="doc-biblioref">2003</a>)</span> propose to standardize the weight by
<span class="math display">\[
\hat\tau = \sum_{i:t_i =1} \frac{y_i}{\hat \pi(\mathbf x_i)}\Big/\sum_{i:t_i =1}\frac{1}{\hat \pi(\mathbf x_i)} - \sum_{i:t_i =-1} \frac{y_i}{1-\hat \pi(\mathbf x_i)}\Big/\sum_{i:t_i =-1}\frac{1}{1-\hat \pi(\mathbf x_i)}.
\]</span>
In our simulation studies, to distinguish from HT, IPW is refered to this standardized weighting method.

Similarly, for ATT, it can be estimated by
<span class="math display">\[
\begin{aligned}
\tau^{\text{ATT}}
=&amp; E[Y^{(1)} - Y^{(-1)}\mid T = 1] = E[Y^{(1)}\mid T=1] - E[Y^{(-1)}\mid T=1]\\
=&amp; \frac{1}{\pi}\left(E[1[T=1]Y^{(1)}]+E[1[T=-1]Y^{(-1)}] - E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right]\right) \\
=&amp; \frac{1}{\pi}E\left[\left(1-\frac{1[T = -1]}{1-\pi(\mathbf X)}\right)Y\right] 
= \frac{1}{\pi}E\left[\left(\frac{1[T = 1]-\pi(\mathbf X)}{1-\pi(\mathbf X)}\right)Y\right] \\
=&amp; \frac{1}{n_1}\left(\sum_{i:t_i=1}y_i - \sum_{i:t_i=-1}\frac{\pi(\mathbf x_i)}{1-\pi(\mathbf x_i)}y_i\right) \quad \text{for finite sample estimation}
\end{aligned} 
\]</span>
where the second equation comes from:
<span class="math display">\[
\begin{aligned}
&amp; E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right] = E[Y^{(-1)}] = E[Y^{(-1)}\mid T=1]\Pr(T=1) + E[Y^{(-1)}\mid T=-1]\Pr(T=-1)\ \\
\Rightarrow &amp; E[Y^{(-1)}\mid T=1] = \frac{1}{\pi}\left(E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right] - E[1[T=-1]Y^{(-1)}]\right)
\end{aligned}
\]</span></p>
</div>
<div id="augmented-inverse-probability-weighting-aipw" class="section level3">
<h3>Augmented Inverse Probability Weighting (AIPW) </h3>
<p><span class="citation">Robins, Rotnitzky, and Zhao (<a href="#ref-robins1995analysis" role="doc-biblioref">1995</a>)</span> augmented the IPW by a weighted average of the outcome model
<span class="math display">\[\begin{align}
\tau^{\text{AIPW}} =&amp; \underbrace{E\left[\frac{1[T = 1]Y}{\pi(\mathbf X)}\right] - E\left[\frac{1[T = -1]Y}{1-\pi(\mathbf X)}\right]}_\text{IPW} \nonumber \\
&amp;- \underbrace{E\left[\frac{1[T=1] -\pi(\mathbf X)}{\pi(\mathbf X)}\mu_1(\mathbf X) + \frac{1[T=1] -\pi(\mathbf X)}{1-\pi(\mathbf X)} \mu_{-1}(\mathbf X)\right]}_\text{Augmentation} \label{eq:AIPW}
\end{align}\]</span>
This AIPW is called “doubly robust” because it is consistent as long as either the treatment assignment mechanism or the outcome model is correctly specified <span class="citation">(Kurz <a href="#ref-kurz2021augmented" role="doc-biblioref">2021</a>)</span>. If the propensity score is correctly specified, then <span class="math inline">\(E(1[T=t] - \Pr(T=t\mid \mathbf X)) = E\left[E(1[T=t] - \Pr(T=t\mid \mathbf X)\mid \mathbf X)\right] = 0\)</span> which simplifies AIPW to IPW no matter whether response surface functions <span class="math inline">\(\mu_T()\)</span> are correct. On the other hand, if response model <span class="math inline">\(\mu_T()\)</span> are correctly specified but propensity score is not correctly estimated, AIPW reduces to S- or T-learner (definitions are in the next section. Proof in Appendix )</p>
</div>
</div>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<div id="proof-of-ipw" class="section level2">
<h2>Proof of IPW </h2>
<p><span class="math display">\[
\begin{aligned}
&amp; E\left[\frac{1[T = t]Y}{\Pr(T=t \mid \mathbf X)}\right] \\
=&amp; E\left[ E\left(\frac{1[T = t]Y}{\Pr(T=t \mid \mathbf X)} \mid \mathbf X\right)\right] \\
=&amp; E\left[ \frac{E\left(1[T = t]\sum_c1[T=c]Y^{(c)} \mid \mathbf X \right)}{\Pr(T=t \mid \mathbf X)} \right] \\
=&amp; E\left[ \frac{\sum_a E\left(1[T = t]\sum_c1[T=c]Y^{(c)} \mid T = a, \mathbf X \right)\Pr(T = a\mid \mathbf X)}{\Pr(T=t \mid \mathbf X)} \right] \\
=&amp; E\left[ \frac{E\left(Y^{(t)} \mid T = t, \mathbf X \right)\Pr(T = t\mid \mathbf X)}{\Pr(T=t \mid \mathbf X)} \right] \\
=&amp; E\left[ E\left(Y^{(t)} \mid T = t, \mathbf X \right) \right] \\
=&amp; E\left[ E\left(Y^{(t)} \mid \mathbf X \right) \right] \text{ (unconfoundedness)} \\
=&amp; E\left[ Y^{(t)} \right].
\end{aligned}
\]</span></p>
</div>
<div id="proof-of-aipw" class="section level2">
<h2>Proof of AIPW </h2>
<p>To demonstrate that AIPW can be consistent even when propensity score is not correctly specified, we only need to focus on the general element <span class="math inline">\(E\left[\frac{1[T = t]Y}{\pi^{(t)}(\mathbf X)} - \frac{1[T=t] -\pi^{(t)}(\mathbf X)}{\pi^{(t)}(\mathbf X)} \mu_t(\mathbf X)\right]\)</span>.</p>
<p>When propensity score is wrong, i.e., <span class="math inline">\(\pi^{(t)}(\mathbf X)\neq \Pr(T = t\mid \mathbf X)\)</span> but response surface estimation is correct, i.e., <span class="math inline">\(\mu_t(\mathbf X) = E(Y^{(t)} \mid \mathbf X)\)</span>,
<span class="math display">\[
\begin{aligned}
&amp; E\left[\frac{1[T = t]Y}{\pi^{(t)}(\mathbf X)} - \frac{1[T=t]}{\pi^{(t)}(\mathbf X)} \mu_t(\mathbf X) + \mu_t(\mathbf X)\right] \\
=&amp; E\left[E\left(\frac{1[T = t]Y}{\pi^{(t)}(\mathbf X)} - \frac{1[T=t]}{\pi^{(t)}(\mathbf X)} \mu_t(\mathbf X) + \mu_t(\mathbf X) \mid \mathbf X \right)\right] \\
=&amp; E\left[ \frac{E\left(1[T = t](Y - \mu_t(\mathbf X)) \mid \mathbf X\right)}{\pi^{(t)}(\mathbf X)}\right] + E[E(Y^{(t)} \mid \mathbf X)] \\
=&amp; E\left[ \frac{\sum_a E\left(1[T = t](Y - \mu_t(\mathbf X)) \mid T = a,\mathbf X\right)\Pr(T = a \mid \mathbf X)}{\pi^{(t)}(\mathbf X)}\right] + E\left[Y^{(t)}\right]  \\
=&amp; E\left[ \frac{E\left(Y - \mu_t(\mathbf X) \mid T = t,\mathbf X\right)\Pr(T = t \mid \mathbf X)}{\pi^{(t)}(\mathbf X)}\right] + E\left[Y^{(t)}\right]  \\
=&amp; E\left[ \frac{\Pr(T = t \mid \mathbf X)}{\pi^{(t)}(\mathbf X)}\underbrace{\left(E\left(Y^{(t)} \mid \mathbf X\right) - \mu_t(\mathbf X) \right)}_{=0} \right] + E\left[Y^{(t)}\right]  \text{ (unconfoundedness)} \\
=&amp; E\left[Y^{(t)}\right] .
\end{aligned}
\]</span>
Therefore, under this situation, AIPW in equation () becomes <span class="math inline">\(\tau^{AIPW}=E\left[Y^{(1)}\right] - E\left[Y^{(-1)}\right] = \tau\)</span>.</p>
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>

<p></p>
<div id="refs" class="references">
<div id="ref-athey2016recursive">
<p>Athey, Susan, and Guido Imbens. 2016. “Recursive Partitioning for Heterogeneous Causal Effects.” <em>Proceedings of the National Academy of Sciences</em> 113 (27): 7353–60.</p>
</div>
<div id="ref-austin2011introduction">
<p>Austin, Peter C. 2011. “An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.” <em>Multivariate Behavioral Research</em> 46 (3): 399–424.</p>
</div>
<div id="ref-austin2014comparison">
<p>———. 2014. “A Comparison of 12 Algorithms for Matching on the Propensity Score.” <em>Statistics in Medicine</em> 33 (6): 1057–69.</p>
</div>
<div id="ref-austin2015moving">
<p>Austin, Peter C, and Elizabeth A Stuart. 2015. “Moving Towards Best Practice When Using Inverse Probability of Treatment Weighting (Iptw) Using the Propensity Score to Estimate Causal Treatment Effects in Observational Studies.” <em>Statistics in Medicine</em> 34 (28): 3661–79.</p>
</div>
<div id="ref-bang2005doubly">
<p>Bang, Heejung, and James M Robins. 2005. “Doubly Robust Estimation in Missing Data and Causal Inference Models.” <em>Biometrics</em> 61 (4): 962–73.</p>
</div>
<div id="ref-gutman2013robust">
<p>Gutman, Roee, and Donald B Rubin. 2013. “Robust Estimation of Causal Effects of Binary Treatments in Unconfounded Studies with Dichotomous Outcomes.” <em>Statistics in Medicine</em> 32 (11): 1795–1814.</p>
</div>
<div id="ref-hahn1998role">
<p>Hahn, Jinyong. 1998. “On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects.” <em>Econometrica</em>, 315–31.</p>
</div>
<div id="ref-hirano2003efficient">
<p>Hirano, Keisuke, Guido W Imbens, and Geert Ridder. 2003. “Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score.” <em>Econometrica</em> 71 (4): 1161–89.</p>
</div>
<div id="ref-horvitz1952generalization">
<p>Horvitz, Daniel G, and Donovan J Thompson. 1952. “A Generalization of Sampling Without Replacement from a Finite Universe.” <em>Journal of the American Statistical Association</em> 47 (260): 663–85.</p>
</div>
<div id="ref-imai2014covariate">
<p>Imai, Kosuke, and Marc Ratkovic. 2014. “Covariate Balancing Propensity Score.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 76 (1): 243–63.</p>
</div>
<div id="ref-imbens2015causal">
<p>Imbens, Guido W, and Donald B Rubin. 2015. <em>Causal Inference in Statistics, Social, and Biomedical Sciences</em>. Cambridge University Press.</p>
</div>
<div id="ref-kurz2021augmented">
<p>Kurz, Christoph F. 2021. “Augmented Inverse Probability Weighting and the Double Robustness Property.” <em>Medical Decision Making</em>, 0272989X211027181.</p>
</div>
<div id="ref-kunzel2019metalearners">
<p>Künzel, Sören R, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. 2019. “Metalearners for Estimating Heterogeneous Treatment Effects Using Machine Learning.” <em>Proceedings of the National Academy of Sciences</em> 116 (10): 4156–65.</p>
</div>
<div id="ref-neyman1923application">
<p>Neyman, Jerzy S. 1923. “On the Application of Probability Theory to Agricultural Experiments. Essay on Principles. Section 9.(tlanslated and Edited by Dm Dabrowska and Tp Speed, Statistical Science (1990), 5, 465-480).” <em>Annals of Agricultural Sciences</em> 10: 1–51.</p>
</div>
<div id="ref-NieQuasi2020">
<p>Nie, X, and S Wager. 2020. “Quasi-oracle estimation of heterogeneous treatment effects.” <em>Biometrika</em> 108 (2): 299–319. <a href="https://doi.org/10.1093/biomet/asaa076">https://doi.org/10.1093/biomet/asaa076</a>.</p>
</div>
<div id="ref-powers2018some">
<p>Powers, Scott, Junyang Qian, Kenneth Jung, Alejandro Schuler, Nigam H Shah, Trevor Hastie, and Robert Tibshirani. 2018. “Some Methods for Heterogeneous Treatment Effect Estimation in High Dimensions.” <em>Statistics in Medicine</em> 37 (11): 1767–87.</p>
</div>
<div id="ref-qi2020multi">
<p>Qi, Zhengling, Dacheng Liu, Haoda Fu, and Yufeng Liu. 2020. “Multi-Armed Angle-Based Direct Learning for Estimating Optimal Individualized Treatment Rules with Various Outcomes.” <em>Journal of the American Statistical Association</em> 115 (530): 678–91.</p>
</div>
<div id="ref-qian2011performance">
<p>Qian, Min, and Susan A Murphy. 2011. “Performance Guarantees for Individualized Treatment Rules.” <em>Annals of Statistics</em> 39 (2): 1180.</p>
</div>
<div id="ref-robins1995analysis">
<p>Robins, James M, Andrea Rotnitzky, and Lue Ping Zhao. 1995. “Analysis of Semiparametric Regression Models for Repeated Outcomes in the Presence of Missing Data.” <em>Journal of the American Statistical Association</em> 90 (429): 106–21.</p>
</div>
<div id="ref-rosenbaum1983central">
<p>Rosenbaum, Paul R, and Donald B Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” <em>Biometrika</em> 70 (1): 41–55.</p>
</div>
<div id="ref-rosenbaum1984reducing">
<p>———. 1984. “Reducing Bias in Observational Studies Using Subclassification on the Propensity Score.” <em>Journal of the American Statistical Association</em> 79 (387): 516–24.</p>
</div>
<div id="ref-rubin:1985">
<p>Rubin, D. B. 1985. “The Use of Propensity Scores in Applied Bayesian Inference.” In <em>Bayesian Statistics 2</em>, 463–72. North-Holland/Elsevier (Amsterdam; New York).</p>
</div>
<div id="ref-rubin1974estimating">
<p>Rubin, Donald B. 1974. “Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.” <em>Journal of Educational Psychology</em> 66 (5): 688.</p>
</div>
<div id="ref-rubin1979using">
<p>———. 1979. “Using Multivariate Matched Sampling and Regression Adjustment to Control Bias in Observational Studies.” <em>Journal of the American Statistical Association</em> 74 (366a): 318–28.</p>
</div>
<div id="ref-rubin1980randomization">
<p>———. 1980. “Randomization Analysis of Experimental Data: The Fisher Randomization Test Comment.” <em>Journal of the American Statistical Association</em> 75 (371): 591–93.</p>
</div>
<div id="ref-van2006targeted">
<p>Van Der Laan, Mark J, and Daniel Rubin. 2006. “Targeted Maximum Likelihood Learning.” <em>The International Journal of Biostatistics</em> 2 (1).</p>
</div>
<div id="ref-zhao2012estimating">
<p>Zhao, Yingqi, Donglin Zeng, A John Rush, and Michael R Kosorok. 2012. “Estimating Individualized Treatment Rules Using Outcome Weighted Learning.” <em>Journal of the American Statistical Association</em> 107 (499): 1106–18.</p>
</div>
<div id="ref-zhou2017residual">
<p>Zhou, Xin, Nicole Mayer-Hamblett, Umer Khan, and Michael R Kosorok. 2017. “Residual Weighted Learning for Estimating Individualized Treatment Rules.” <em>Journal of the American Statistical Association</em> 112 (517): 169–87.</p>
</div>
</div>
</div>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2019 -
        
        2022
         Junyi Zhou 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    
      
      <script src="/js/coder.min.235666b114443867d43eeb5799d51f6252965e5163f338285e113fa381d3d27e.js" integrity="sha256-I1ZmsRREOGfUPutXmdUfYlKWXlFj8zgoXhE/o4HT0n4="></script>
    

    

    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4GM2YKH34F"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-4GM2YKH34F', { 'anonymize_ip': false });
}
</script>


    

    

    

    

    

    
  </body>

</html>
